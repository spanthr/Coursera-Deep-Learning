{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "computerVision_Editting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MskBWPcUyqyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "c7643289-a7c5-4ca1-ffb6-a71c100d5abc"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0  # Remove normalizing\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),     #comment this to see\n",
        "                                    #tf.keras.layers.Dense(512, activation=tf.nn.relu),       # Adding additional layers\n",
        "                                    #tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                    #tf.keras.layers.Dense(1024, activation=tf.nn.relu),         # changed  1024 ::: 128\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])       # Change 10 to 5\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=1)     # more epochs or less\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2601\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1335\n",
            "[3.1489481e-06 9.5087040e-09 1.7693292e-05 2.5026311e-04 1.1258811e-08\n",
            " 2.1794929e-06 8.9976214e-11 9.9966586e-01 8.8723345e-06 5.1984483e-05]\n",
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Smc5vjB0Fy6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ec600ae-846e-4daa-b6b4-1d47df26e129"
      },
      "source": [
        "\"\"\"More accurate but take longer\"\"\"\n",
        "\"\"\"Adding neurons makes more calculations and slow process but impactful that is\n",
        "more accurate results. But it certainly doesnot mean more is better\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Adding neurons makes more calculations and slow process but impactful that is\\nmore accurate results. But it certainly doesnot mean more is better'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8eGg5d70eZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "902911ed-0c51-4aff-c3c2-b588f75db610"
      },
      "source": [
        "\"\"\"What would happen if you remove the Flatten() layer.\n",
        "get an error about the shape of the data.\n",
        " It may seem vague right now, but it reinforces the rule of thumb that the first\n",
        "  layer in your network should be the same shape as your data.\n",
        "  Right now our data is 28x28 images, and 28 layers of 28 neurons would be infeasible, \n",
        "  so it makes more sense to 'flatten' that 28,28 into a 784x1.\n",
        "   Instead of wriitng all the code to handle that ourselves,\n",
        "  we add the Flatten() layer at the begining, and when the arrays are loaded \n",
        "  into the model later, they'll automatically be flattened for us.\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"What would happen if you remove the Flatten() layer.\\nget an error about the shape of the data.\\n It may seem vague right now, but it reinforces the rule of thumb that the first\\n  layer in your network should be the same shape as your data.\\n  Right now our data is 28x28 images, and 28 layers of 28 neurons would be infeasible, \\n  so it makes more sense to 'flatten' that 28,28 into a 784x1.\\n   Instead of wriitng all the code to handle that ourselves,\\n  we add the Flatten() layer at the begining, and when the arrays are loaded \\n  into the model later, they'll automatically be flattened for us.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV81QKfV0_Vs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9626fb3d-6cce-413d-ad25-9e1eb15ba8d4"
      },
      "source": [
        "\"\"\"\n",
        "Consider the final (output) layers. Why are there 10 of them? What would happen \n",
        "if you had a different amount than 10? For example, try training the network\n",
        " with 5\n",
        "\n",
        "You get an error as soon as it finds an unexpected value. Another rule of thumb \n",
        "-- the number of neurons in the last layer should match the number of classes\n",
        " you \n",
        "are classifying for. In this case it's the digits 0-9, so there are 10 of them, \n",
        "hence you should have 10 neurons in your final layer.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nConsider the final (output) layers. Why are there 10 of them? What would happen \\nif you had a different amount than 10? For example, try training the network\\n with 5\\n\\nYou get an error as soon as it finds an unexpected value. Another rule of thumb \\n-- the number of neurons in the last layer should match the number of classes\\n you \\nare classifying for. In this case it's the digits 0-9, so there are 10 of them, \\nhence you should have 10 neurons in your final layer.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-4jGyGlFSCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Exrecise 6\n",
        "Consider the impact of training for more or less epochs. Why do you think that \n",
        "would be the case?\n",
        "\n",
        "Try 15 epochs -- you'll probably get a model with a much better loss than the\n",
        " one with 5 \n",
        "Try 30 epochs -- you might see the loss value stops decreasing, and sometimes\n",
        " increases.\n",
        " This is a side effect of something called 'overfitting' which you can learn \n",
        " about [somewhere]\n",
        "  and it's something you need to keep an eye out for when training neural \n",
        "networks. There's no point in wasting your time training if you aren't improving\n",
        "your loss, right! :)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J1O3bc5FcsT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cf465745-0aa3-4ecf-e069-ec1e8678bf42"
      },
      "source": [
        "\"\"\" Exercise 7 remove normalizing\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "' Exercise 7 remove normalizing\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZexjAQaCFie2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}